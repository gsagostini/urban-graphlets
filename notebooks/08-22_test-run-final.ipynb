{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f434674f-9d10-4a76-aafb-c11d7bf91712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa265d8-8f4a-4e32-b9bc-47b2001b3cbc",
   "metadata": {},
   "source": [
    "# Few-City Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b783d1d-1c61-461c-9dbf-f591ea2cca15",
   "metadata": {},
   "source": [
    "This notebook executes the pipeline and illustrates the methodology for the case of a few cities. We follow the structure:\n",
    "\n",
    "![methodology](figures/methodology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d7159-c828-45f0-81a2-89da177a4a17",
   "metadata": {},
   "source": [
    "## 1. Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107aa6df-c226-4a08-8065-61514ff3aac2",
   "metadata": {},
   "source": [
    "For this example, we will bypass the list of cities provided on the .csv file and simply define what cities we would like to treat at this point. We also get the projection of the GHSL data, which will be used to standardized all GeoDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a772d610-e09d-45a0-8a9c-760301773cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vars import ghsl_crs, test_cities\n",
    "cities, countries = zip(*test_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d47757-afc6-4ffd-9b08-cab5b9fc0b67",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a55cbd-f30c-4560-9295-029cd455ff0c",
   "metadata": {},
   "source": [
    "We proceed to obtain the data we will need throughout the experiments. The data will be saved in the directory ../data/test-run , and any files pre-populating the directory will be overrun by this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff60ade-d090-4abf-986b-2cb2d8c98290",
   "metadata": {},
   "source": [
    "### 2.1. Boundary Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b90e02-6a0b-4d1d-915c-0bde9fa2b6e4",
   "metadata": {},
   "source": [
    "This is a temporary fix. I want to get the boundary polygons from the GHSL SMOD dataset but the projection they use has a bug and I cannot seem to read the GeoPackage yet. I will continue to work on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a5864a-b621-4452-bf30-745ef85b8d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 40.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem with  Sidney, Australia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:13,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from src import get_boundary as gb\n",
    "boundaries_dict = gb.get_boundaries(cities, countries, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029cf4a-58b9-410b-bf22-9aac07d3bbf7",
   "metadata": {},
   "source": [
    "### 2.2. Street Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae635cfc-cc74-413e-80b2-2204e7da0ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 12/12 [41:53<00:00, 209.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from src import get_graph as gg\n",
    "graphs_dict = gg.get_graphs(boundaries_dict, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e2886-9825-4f2e-82fa-65148637cd90",
   "metadata": {},
   "source": [
    "### 2.3. Graphlet Degree Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871169e-3f80-4303-b639-87f7e8dda981",
   "metadata": {},
   "source": [
    "We will simultaneously get the GDM dictionary and the dictionary of nodes GeoDataFrames (which will be used per se):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bf1a90-6f14-4c76-8ba3-bc6184532d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:33<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from src import get_GDM as ggdm\n",
    "GDMs_dict, nodes_gdfs_dict = ggdm.get_GDMs(graphs_dict, test=True, get_nodes_gdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e09d43-7ddd-4820-9855-ed8f42957e4b",
   "metadata": {},
   "source": [
    "## 3. Obtaining the Graphlet Correlation Matrix of each GHSL tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98dca5a-7f61-48e7-8e93-dfa8de4c1472",
   "metadata": {},
   "source": [
    "We will get the data for each of the GHSL tiles and turn that into a geodataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c993e95b-e605-4d07-a146-6c0e988cb8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 12/12 [2:24:59<00:00, 724.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from src import get_GCM as ggcm\n",
    "full_gdf = ggcm.get_ghsl_geodataframe(nodes_gdfs_dict, boundaries_dict, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba7294-c169-4402-9fdf-0ec30409a45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
